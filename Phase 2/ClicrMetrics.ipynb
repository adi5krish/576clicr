{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ClicrMetrics.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUNA2ZS83Aut",
        "colab_type": "text"
      },
      "source": [
        "Script to find the exact match and F1 scores on **version 1 of clicr**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKLWX8qG2FK7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "10265c10-0868-469b-e168-c7b95bc7a432"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/'My Drive'"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n",
            "/gdrive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYfZr2Wx2IT7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "733a2860-ce01-48f3-f94d-304175c3a4d6"
      },
      "source": [
        "import sys\n",
        "sys.path.append('Data')\n",
        "%tensorflow_version 1.x"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmS924Rt1107",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import json\n",
        "import datetime\n",
        "import os\n",
        "import pprint\n",
        "import random\n",
        "import string\n",
        "\n",
        "def normalize_answer(s, lemmatizer_comm=None):\n",
        "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "    def remove_articles(text):\n",
        "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    def lemma(text):\n",
        "        if lemmatizer_comm is None:\n",
        "            lemmatized = text\n",
        "        else:\n",
        "            lemmatized = lemmatize(text, lemmatizer_comm)\n",
        "        return lemmatized\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(lemma(s)))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLEnTUKR12q1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_json(filename):\n",
        "    with open(filename) as in_f:\n",
        "        return json.load(in_f)\n",
        "\n",
        "DATA_KEY = \"data\"\n",
        "VERSION_KEY = \"version\"\n",
        "DOC_KEY = \"document\"\n",
        "QAS_KEY = \"qas\"\n",
        "ANS_KEY = \"answers\"\n",
        "TXT_KEY = \"text\"  # the text part of the answer\n",
        "ORIG_KEY = \"origin\"\n",
        "ID_KEY = \"id\"\n",
        "TITLE_KEY = \"title\"\n",
        "CONTEXT_KEY = \"context\"\n",
        "SOURCE_KEY = \"source\"\n",
        "QUERY_KEY = \"query\"\n",
        "CUI_KEY = \"cui\"\n",
        "SEMTYPE_KEY = \"sem_type\"\n",
        "ISIMPOSSIBLE_KEY = \"is_impossible\"\n",
        "PLACEHOLDER_KEY = \"@placeholder\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74gw_GhM14C4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def f1_score(prediction, ground_truth, comm=None):\n",
        "    prediction_tokens = normalize_answer(prediction, comm).split()\n",
        "    ground_truth_tokens = normalize_answer(ground_truth, comm).split()\n",
        "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
        "    num_same = sum(common.values())\n",
        "    if num_same == 0:\n",
        "        return 0\n",
        "    precision = 1.0 * num_same / len(prediction_tokens)\n",
        "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "\n",
        "def exact_match_score(prediction, ground_truth, comm=None):\n",
        "    return normalize_answer(prediction, comm) == normalize_answer(ground_truth, comm)\n",
        "\n",
        "\n",
        "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths, comm=None):\n",
        "    scores_for_ground_truths = []\n",
        "    for ground_truth in ground_truths:\n",
        "        score = metric_fn(prediction, ground_truth, comm)\n",
        "        scores_for_ground_truths.append(score)\n",
        "    return max(scores_for_ground_truths)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XG_PczSO15i5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import  Counter\n",
        "\n",
        "def evaluate(dataset, predictions, lemmatizer_path=None, extended=False, embeddings_file=None, downcase=False):\n",
        "    data = dataset[DATA_KEY]\n",
        "    comm = lemmatizer(lemmatizer_path) if lemmatizer_path else None\n",
        "    f1 = exact_match = total = 0\n",
        "\n",
        "    n_unanswered = 0\n",
        "    datum_count = 0\n",
        "    \n",
        "    for datum in data:\n",
        "      for qa in datum[DOC_KEY][QAS_KEY]:\n",
        "          ground_truths = []\n",
        "          total += 1\n",
        "          if qa[ID_KEY] not in predictions:\n",
        "              n_unanswered += 1\n",
        "              continue    \n",
        "          ground_truths = list(map(lambda x: x[TXT_KEY], qa[ANS_KEY]))\n",
        "          prediction = predictions[qa[ID_KEY]]\n",
        "          exact_match += metric_max_over_ground_truths(\n",
        "              exact_match_score, prediction, ground_truths, comm=comm)\n",
        "          f1 += metric_max_over_ground_truths(\n",
        "              f1_score, prediction, ground_truths, comm=comm)\n",
        "      datum_count += 1\n",
        "    \n",
        "    print(\"There were {} unanswered instances\".format(n_unanswered))\n",
        "    exact_match = 100.0 * exact_match / total\n",
        "    print(total)\n",
        "    f1 = 100.0 * f1 / total\n",
        "    # assert exact_match <= f1\n",
        "    scores = {'exact_match': exact_match, 'f1': f1}\n",
        "    return scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdi2IwN917ZF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_scores(scores):\n",
        "    \"\"\"\n",
        "    :param scores: {\"method1\": score, ...}\n",
        "    \"\"\"\n",
        "    print(\"{}\\t{:.1f}\".format(\"exact_match\", scores[\"exact_match\"]))\n",
        "    print(\"{}\\t{:.1f}\".format(\"f1\", scores[\"f1\"]))\n",
        "\n",
        "    for method, score in sorted(scores.items()):\n",
        "        if method == \"exact_match\" or method == \"f1\":\n",
        "            continue\n",
        "        else:\n",
        "            print(\"{}\\t{:.3f}\".format(method, score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9DQmsFZ18vh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "1d19d422-56fc-4936-eeaf-857f3dd023dc"
      },
      "source": [
        "dataset = load_json('New-Output-III/bert_dev.json')       # path for the development json file\n",
        "predictions = load_json('Data/output_biobertlarge_summarized_1.0_predictions.json')   # path for the predictions file\n",
        "print_scores(evaluate(dataset, predictions))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There were 1048 unanswered instances\n",
            "3558\n",
            "exact_match\t30.0\n",
            "f1\t31.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT0qYuR52rnS",
        "colab_type": "text"
      },
      "source": [
        "Command to run Evaluation file that returns the Exact Match and F1 scores for **v2** **of** **clicr**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfoknMs32pKJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "769b7345-2742-446e-847d-2bbe474ea2dc"
      },
      "source": [
        "! python Data/eval2.0.py New-Output-IV/bert_test.json Data/output_biobertbase_pubmed_pmc_2.0_test_predictions.json"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"exact\": 52.19933184855234,\n",
            "  \"f1\": 53.037111753008176,\n",
            "  \"total\": 7184,\n",
            "  \"HasAns_exact\": 52.19933184855234,\n",
            "  \"HasAns_f1\": 53.037111753008176,\n",
            "  \"HasAns_total\": 7184\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWw-cjyv3NTL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}